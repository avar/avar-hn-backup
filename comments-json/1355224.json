{
  "by" : "avar",
  "id" : 1355224,
  "parent" : 1354847,
  "text" : "It still handles them poorly by default. You can do things like edit .gitattributes to say that e.g. a *.binary file should be treated specially, but Git's still not a good system for e.g. archiving HD video.<p>Some of this can be fixed, but a lot of it is probably not going to change. When you git-add something it has to checksum the old data + new data. That's going to be a pretty expensive operation when you have 50TB of data.<p>There's no DVCS that I'm aware of that handles large binary files as well as say Perforce or Subversion.<p>Check out this project for more info on tuning Git for big files: <a href=\"http://caca.zoy.org/wiki/git-bigfiles\" rel=\"nofollow\">http://caca.zoy.org/wiki/git-bigfiles</a>",
  "time" : 1274123618,
  "type" : "comment"
}
